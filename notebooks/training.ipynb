{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed2e8f5",
      "metadata": {
        "id": "6ed2e8f5",
        "outputId": "e09b9668-bde8-45b0-91d2-1e7f4ff1bd01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now in: c:\\Users\\faizan\\Desktop\\EAST\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir(\"..\")  # move up from notebooks to repo root\n",
        "print(\"Now in:\", os.getcwd())\n",
        "# Add project root to sys.path so 'src' can be imported\n",
        "import sys\n",
        "project_root = os.path.abspath(os.getcwd())\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80fded25",
      "metadata": {
        "id": "80fded25",
        "outputId": "68fa4d87-b1d6-479d-9e72-29f6985fc3af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\faizan\\Desktop\\EAST\n",
            "Dataset length: 1000\n",
            "First 3 image paths: ['data/icdar2015/train_images\\\\img_1.jpg', 'data/icdar2015/train_images\\\\img_10.jpg', 'data/icdar2015/train_images\\\\img_100.jpg']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "from src.dataset import EASTDataset\n",
        "\n",
        "ds = EASTDataset(\"data/icdar2015/train_images\", \"data/icdar2015/train_maps\", size=512)\n",
        "\n",
        "print(\"Dataset length:\", len(ds))\n",
        "print(\"First 3 image paths:\", ds.img_paths[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cae46cd",
      "metadata": {
        "id": "7cae46cd",
        "outputId": "cd658b8f-b14f-43e2-e7c4-eadcaffa6f65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images : torch.Size([4, 3, 512, 512])\n",
            "Scores : torch.Size([4, 1, 512, 512])\n",
            "Geos   : torch.Size([4, 8, 512, 512])\n",
            "Nmaps  : torch.Size([4, 1, 512, 512])\n",
            "Image range: -2.1179039478302 to 2.640000104904175\n",
            "Score unique values: tensor([0., 1.])\n",
            "Geo stats: mean 0.0132293701171875 std 7.47803258895874\n",
            "Nmap stats (on positive pixels): mean 127.58235168457031 std 35.18562698364258\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from src.dataset import EASTDataset\n",
        "\n",
        "# Create a small dataset instance\n",
        "train_dataset = EASTDataset(\n",
        "    img_dir=\"data/icdar2015/train_images\",\n",
        "    map_dir=\"data/icdar2015/train_maps\",\n",
        "    size=512,              # keep same as paper\n",
        "    training=True          # enable random cropping/augmentation\n",
        ")\n",
        "\n",
        "# Try a small dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# Fetch one batch, now including the nmap\n",
        "imgs, scores, geos, nmaps = next(iter(train_loader))\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Images :\", imgs.shape)    # expect [B, 3, 512, 512]\n",
        "print(\"Scores :\", scores.shape)  # expect [B, 1, 512, 512]\n",
        "print(\"Geos   :\", geos.shape)    # expect [B, 8, 512, 512]\n",
        "print(\"Nmaps  :\", nmaps.shape)   # expect [B, 1, 512, 512]\n",
        "\n",
        "# Check value ranges and properties\n",
        "print(\"Image range:\", imgs.min().item(), \"to\", imgs.max().item())\n",
        "print(\"Score unique values:\", torch.unique(scores))\n",
        "print(\"Geo stats: mean\", geos.mean().item(), \"std\", geos.std().item())\n",
        "\n",
        "# Verify nmap has non-zero values where score is positive\n",
        "print(\"Nmap stats (on positive pixels): mean\", nmaps[scores > 0].mean().item(), \"std\", nmaps[scores > 0].std().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74fb9abd",
      "metadata": {
        "id": "74fb9abd",
        "outputId": "58f18bc8-5741-4bc9-daf1-9035fdf58a4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\faizan/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score map: torch.Size([1, 1, 512, 512])\n",
            "Geo map: torch.Size([1, 8, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from src.model import EAST\n",
        "\n",
        "model = EAST(pretrained=False)\n",
        "dummy = torch.randn(1, 3, 512, 512)  # batch of one\n",
        "score, geo = model(dummy)\n",
        "\n",
        "print(\"Score map:\", score.shape)  # expect [1, 1, 512, 512]\n",
        "print(\"Geo map:\", geo.shape)      # expect [1, 8, 512, 512]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e40f27a",
      "metadata": {
        "id": "7e40f27a",
        "outputId": "e6479454-cd29-4047-9300-a8f466f46d6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total loss: 0.1503923237323761\n",
            "Score loss: 0.020851217210292816\n",
            "Geo loss  : 0.12954111397266388\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from src.model import EAST\n",
        "from src.losses import EASTLoss\n",
        "\n",
        "# Assuming imgs, scores, geos, and nmaps are already loaded from your DataLoader test.\n",
        "# If not, you'd need to run the DataLoader test cell again to get these variables.\n",
        "# You might want to move these to a CUDA device if available.\n",
        "# imgs = imgs.to('cuda')\n",
        "# ... and so on for the other tensors\n",
        "\n",
        "# Instantiate the model and criterion\n",
        "model = EAST(pretrained=True)  # Use pretrained=True to follow the paper's method\n",
        "model.eval()  # Set model to evaluation mode for a clean forward pass test\n",
        "\n",
        "# Perform a forward pass to get predictions\n",
        "with torch.no_grad():\n",
        "    pred_score, pred_geo = model(imgs)\n",
        "\n",
        "# Instantiate the loss function\n",
        "criterion = EASTLoss()\n",
        "\n",
        "# Compute the loss using your predictions and ground truth maps\n",
        "total_loss, score_loss, geo_loss = criterion(pred_score, pred_geo, scores, geos, nmaps)\n",
        "\n",
        "# Print the results\n",
        "print(\"Total loss:\", total_loss.item())\n",
        "print(\"Score loss:\", score_loss.item())\n",
        "print(\"Geo loss  :\", geo_loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SKfaizan-786/EAST-text-detection.git\n",
        "%cd EAST-text-detection"
      ],
      "metadata": {
        "id": "OlUpMdyzkInL",
        "outputId": "bd3e001f-1f04-4f39-957d-eed754bc6b19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OlUpMdyzkInL",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EAST-text-detection'...\n",
            "remote: Enumerating objects: 7692, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 7692 (delta 6), reused 14 (delta 4), pack-reused 7670 (from 1)\u001b[K\n",
            "Receiving objects: 100% (7692/7692), 307.15 MiB | 13.25 MiB/s, done.\n",
            "Resolving deltas: 100% (4638/4638), done.\n",
            "Updating files: 100% (7515/7515), done.\n",
            "/content/EAST-text-detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m src.train"
      ],
      "metadata": {
        "id": "q7nW_PsYlsEC",
        "outputId": "7030f4f8-6225-4169-ea3e-2e3e86978320",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "q7nW_PsYlsEC",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 900,  Val size: 100\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 118MB/s]\n",
            "Training on 900 images and validating on 100 images for 100 epochs\n",
            "Epoch 1/100: 100%|█| 225/225 [02:42<00:00,  1.39it/s, total=0.1345, score=0.0551\n",
            "Epoch 1 | train loss 0.0950 | val loss 0.1001\n",
            "✅ New best model saved with val loss: 0.1001\n",
            "Epoch 2/100: 100%|█| 225/225 [02:38<00:00,  1.42it/s, total=0.1810, score=0.0808\n",
            "Epoch 2 | train loss 0.0945 | val loss 0.0834\n",
            "✅ New best model saved with val loss: 0.0834\n",
            "Epoch 3/100: 100%|█| 225/225 [02:31<00:00,  1.49it/s, total=0.0751, score=0.0176\n",
            "Epoch 3 | train loss 1476.6351 | val loss 0.0967\n",
            "Epoch 4/100: 100%|█| 225/225 [02:24<00:00,  1.56it/s, total=0.1641, score=0.0456\n",
            "Epoch 4 | train loss 0.0978 | val loss 0.0920\n",
            "Epoch 5/100: 100%|█| 225/225 [02:20<00:00,  1.60it/s, total=0.4534, score=0.0714\n",
            "Epoch 5 | train loss 2411.1412 | val loss 0.3903\n",
            "Epoch 6/100: 100%|█| 225/225 [02:15<00:00,  1.66it/s, total=0.1056, score=0.0251\n",
            "Epoch 6 | train loss 0.1203 | val loss 0.0961\n",
            "Epoch 7/100: 100%|█| 225/225 [02:10<00:00,  1.72it/s, total=0.0997, score=0.0147\n",
            "Epoch 7 | train loss 0.1093 | val loss 0.0893\n",
            "Epoch 8/100: 100%|█| 225/225 [02:06<00:00,  1.78it/s, total=0.0894, score=0.0210\n",
            "Epoch 8 | train loss 0.1080 | val loss 0.0882\n",
            "Epoch 9/100: 100%|█| 225/225 [02:04<00:00,  1.81it/s, total=0.1302, score=0.0809\n",
            "Epoch 9 | train loss 0.1042 | val loss 0.0859\n",
            "Epoch 10/100: 100%|█| 225/225 [02:03<00:00,  1.82it/s, total=0.0900, score=0.048\n",
            "Epoch 10 | train loss 1163.6369 | val loss 0.7827\n",
            "Epoch 11/100: 100%|█| 225/225 [02:04<00:00,  1.81it/s, total=0.2790, score=0.167\n",
            "Epoch 11 | train loss 0.4513 | val loss 0.3153\n",
            "Epoch 12/100: 100%|█| 225/225 [02:03<00:00,  1.82it/s, total=0.2388, score=0.123\n",
            "Epoch 12 | train loss 0.3884 | val loss 0.3607\n",
            "🛑 Early stopping at epoch 12. No improvement for 10 epochs.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}